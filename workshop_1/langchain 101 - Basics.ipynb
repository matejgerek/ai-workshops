{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xTpZKOzC99C_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🦜⛓️ Langchain 101\n",
        "\n",
        "In this notebook, we will explore the basic building blocks of the 🦜⛓️ Langchain framework, which is primarily used to create intelligent agents enabled by large language models. We will focus on practical examples to allow you, the participants, to try building such systems yourself.\n",
        "\n",
        "This notebook is divided into the following main sections:\n",
        "## Setup (2 min)\n",
        "\n",
        "In this section, we will install all the required libraries needed throughout the workshop. Since we will use GPT (from OpenAI) as the main language model throughout the examples, you will be asked to provide the OpenAI API key.\n",
        "\n",
        "## Motivation & Examples (5 min)\n",
        "\n",
        "In this part, we will quickly provide some motivational examples and answer the question: \"What is this good for?\"\n",
        "\n",
        "## Basic Components (15 min)\n",
        "\n",
        "This section will focus on creating the first demonstration of an intelligent agent for personal home automation. We will explain the main components of the agent, such as tools, prompts, toolkits, as well as different types of pre-created and directly accessible agents. At the end of this section, multiple task exercises will be presented to improve our agent.\n",
        "\n",
        "## Main Exercise (15 min)\n",
        "\n",
        "In this section, you will be presented with an open-ended real-life problem that you can try to solve yourself.\n",
        "\n",
        "## Additional Examples (10 min)\n",
        "\n",
        "Here, we will showcase another real-life problem that is quite easily solvable with the use of LangChain.\n",
        "\n",
        "___\n",
        "\n",
        "## Links:\n",
        "- [Langchain documentation](https://python.langchain.com/en/latest)\n",
        "- [Colab basics](https://colab.research.google.com/?utm_source=scs-index)\n",
        "- [Python language quick tutorial]( https://colab.research.google.com/github/data-psl/lectures2020/blob/master/notebooks/01_python_basics.ipynb)"
      ],
      "metadata": {
        "id": "CLDrsfYeUifZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup (2 min)\n",
        "\n",
        "In this section, we will install all the required libraries needed throughout the workshop."
      ],
      "metadata": {
        "id": "C-2Mca_Wl9hB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVurV3yiRFsa"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai google-search-results tiktoken faiss-cpu wikipedia > /dev/null\n",
        "\n",
        "# # needed only in case we want to use custom llm models\n",
        "# !pip install huggingface_hub > /dev/null\n",
        "# !pip install transformers > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we will use GPT (from OpenAI) as the main language model throughout the examples, you will be asked to provide an OpenAI API key.\n",
        "\n",
        "You can get your API key [here](https://platform.openai.com/account/api-keys)"
      ],
      "metadata": {
        "id": "GkOre1LWH7lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass('Enter openAI API key: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz8AOP2kSUfo",
        "outputId": "5bcbf682-57f8-4327-e2fd-2e481436f377"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter openAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example also uses Serper as a way to search the internet for necesarry data.\n",
        "\n",
        "You can create a free account [here](https://serper.dev) and get an API key with 1000 free requests [here](https://serper.dev/api-key)"
      ],
      "metadata": {
        "id": "I9PdB9INIKBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SERPER_API_KEY = getpass('Enter serpi API key: ')\n",
        "os.environ['SERPER_API_KEY'] = SERPER_API_KEY"
      ],
      "metadata": {
        "id": "AsiVEBserZ5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b59067c-79fc-4b3d-9c8f-4b0fc3eb7aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter serpi API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we mount our shared google drive which contains the team directory notion file in CSV format\n",
        "\n",
        "Don't forget to do it with your Sudolabs account :)"
      ],
      "metadata": {
        "id": "Jmg-lQIP_Gkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wscqVhmr_GxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e285c94-9b9f-4016-b5a6-062e6afd9f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation & Examples (5 min)"
      ],
      "metadata": {
        "id": "uwrTGYasnQoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple ReAct Agent"
      ],
      "metadata": {
        "id": "_DLy_D1lePOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "6UYtQXK5eRXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.25)\n",
        "\n",
        "tools = load_tools([\"google-serper\", \"llm-math\"], llm=llm)\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ],
      "metadata": {
        "id": "cLaqkWU9lyKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How old is Waren Buffet and what is his age multiplied by 2?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "52yT93Ual23e",
        "outputId": "4672b749-d4c4-43c9-825a-b8ab71d0c746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out how old Warren Buffet is and then do a calculation.\n",
            "Action: google_serper\n",
            "Action Input: Warren Buffet age\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m92 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now need to multiply 92 by 2\n",
            "Action: Calculator\n",
            "Action Input: 92 * 2\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 184\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Warren Buffet is 92 years old and his age multiplied by 2 is 184.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Warren Buffet is 92 years old and his age multiplied by 2 is 184.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV Agent"
      ],
      "metadata": {
        "id": "e_LFZx4l37-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_csv_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "8YOjRgnu-wz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/Shareddrives/AI Workshops/Workshop 1 - LangChain/data/team_directory.csv'\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "agent = create_csv_agent(llm, filepath, verbose=True)"
      ],
      "metadata": {
        "id": "F0-dsNVb-wu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How many people are from Poprad?\")"
      ],
      "metadata": {
        "id": "-CmZ58vE_UzG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "151669b9-190d-4bc5-d10e-4e0f0b4931ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out how many people are from Poprad in the dataframe.\n",
            "Action: python_repl_ast\n",
            "Action Input: df[df['Location'] == 'Poprad'].shape[0]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m2\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 2 people are from Poprad.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2 people are from Poprad.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"How many different positions are in the engineering department?\")"
      ],
      "metadata": {
        "id": "Y5eYl4iC_UxC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "4b0e5b81-b95f-4723-a656-4760797a0181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out how many different positions are in the engineering department\n",
            "Action: python_repl_ast\n",
            "Action Input: df[df['Department'] == 'Engineering']['Position'].nunique()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m9\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 9\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What is the distribution of positions?\")"
      ],
      "metadata": {
        "id": "uuYi9aYT_Uus",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "outputId": "5805a01d-055d-4da3-d0e7-38227eb88186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find out how many of each position there are.\n",
            "Action: python_repl_ast\n",
            "Action Input: df['Position'].value_counts()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mEngineer II                       13\n",
            "Associate Engineer                12\n",
            "Engineer III                       9\n",
            "Engineer I                         8\n",
            "Product Manager                    8\n",
            "Product Designer                   5\n",
            "Engineering Manager                4\n",
            "Staff Engineer                     3\n",
            "Social Media Marketing Manager     2\n",
            "Senior engineer (deprecated)       2\n",
            "Strategic Partnerships Manager     2\n",
            "Head of Design                     1\n",
            "CTO                                1\n",
            "Social Media Manager               1\n",
            "Community & Space Manager          1\n",
            "Business Development Manager       1\n",
            "Head of Product                    1\n",
            "Associate Product Designer         1\n",
            "Head of HR                         1\n",
            "Head of marketing                  1\n",
            "Senior Engineering Manager         1\n",
            "CEO                                1\n",
            "Chief of Strategy                  1\n",
            "CPO                                1\n",
            "Product Analyst                    1\n",
            "Business Operation Manager         1\n",
            "HR Officer                         1\n",
            "Recruiter/HR Specialist            1\n",
            "Name: Position, dtype: int64\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the distribution of positions.\n",
            "Final Answer: There are 13 Engineer IIs, 12 Associate Engineers, 9 Engineer IIIs, 8 Engineer Is, 8 Product Managers, 5 Product Designers, 4 Engineering Managers, 3 Staff Engineers, 2 Social Media Marketing Managers, 2 Senior Engineers (deprecated), 2 Strategic Partnerships Managers, 1 Head of Design, 1 CTO, 1 Social Media Manager, 1 Community & Space Manager, 1 Business Development Manager, 1 Head of Product, 1 Associate Product Designer, 1 Head of HR, 1 Head of Marketing, 1 Senior Engineering Manager, 1 CEO, 1 Chief of Strategy, 1 CPO, 1 Product Analyst, 1 Business Operation Manager, 1 HR Officer, and 1 Recruiter/HR Specialist.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 13 Engineer IIs, 12 Associate Engineers, 9 Engineer IIIs, 8 Engineer Is, 8 Product Managers, 5 Product Designers, 4 Engineering Managers, 3 Staff Engineers, 2 Social Media Marketing Managers, 2 Senior Engineers (deprecated), 2 Strategic Partnerships Managers, 1 Head of Design, 1 CTO, 1 Social Media Manager, 1 Community & Space Manager, 1 Business Development Manager, 1 Head of Product, 1 Associate Product Designer, 1 Head of HR, 1 Head of Marketing, 1 Senior Engineering Manager, 1 CEO, 1 Chief of Strategy, 1 CPO, 1 Product Analyst, 1 Business Operation Manager, 1 HR Officer, and 1 Recruiter/HR Specialist.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"What is the third most common position in the engineering department?\")"
      ],
      "metadata": {
        "id": "_T7Zw0DW_Ure",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "f01882fd-0432-4ec0-ad7f-8d71144af1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the third most common position in the engineering department\n",
            "Action: python_repl_ast\n",
            "Action Input: df[df['Department'] == 'Engineering']['Position'].value_counts().index[2]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mEngineer III\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Engineer III\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Engineer III'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who has the most senior position in the theexpert project?\")"
      ],
      "metadata": {
        "id": "npBdTPll_Z1U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "11353689-cc74-4a90-e91d-416bc0c726c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the person with the highest position in the theexpert project\n",
            "Action: python_repl_ast\n",
            "Action Input: df[df['Project'] == 'The Expert'].sort_values(by='Position', ascending=False).head(1)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m    Unnamed: 0 Inactive           Name   Department        Position Location  \\\n",
            "36          36       No  Martin Dejmek  Engineering  Staff Engineer   Prague   \n",
            "\n",
            "   LinkedIn                     Email Hobbies Start date GitHub Homepage  \\\n",
            "36      NaN  martindejmek@sudolabs.io     NaN        NaN    NaN      NaN   \n",
            "\n",
            "       Project Twitter Gist Instagram StackOverflow Nickname  \n",
            "36  The Expert     NaN  NaN       NaN           NaN      NaN  \u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Martin Dejmek\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Martin Dejmek'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More agents [here](https://python.langchain.com/en/latest/modules/agents/toolkits.html)"
      ],
      "metadata": {
        "id": "LJ_AVt09_obQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Components (15 min)\n",
        "\n",
        "This section will focus on creating the first demonstration of an intelligent agent for personal home automation. We will explain the main components of the agent, such as tools, prompts, toolkits, as well as different types of pre-created and directly accessible agents. At the end of this section, multiple task exercises will be presented to improve our agent.\n",
        "\n",
        "The most important parts of the Langchain frameworks are:\n",
        "\n",
        "- [**Models**](https://python.langchain.com/en/latest/modules/models/llms.html)\n",
        "- [**Prompts**](https://python.langchain.com/en/latest/modules/prompts.html)\n",
        "    - [Conceptual guide](https://docs.langchain.com/docs/components/prompts)\n",
        "    - [Prompt templates](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/getting_started.html)\n",
        "- [**Chains**](https://python.langchain.com/en/latest/modules/chains.html)\n",
        "    - [Conceptual Guide](https://docs.langchain.com/docs/components/chains)\n",
        "    - [Intro to chains](https://python.langchain.com/en/latest/modules/chains/getting_started.html)\n",
        "- [**Agents**](https://python.langchain.com/en/latest/modules/agents.html)\n",
        "    - [Intro into agents](https://python.langchain.com/en/latest/modules/agents.html)\n",
        "    - [Agent types](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)\n",
        "- [**Indexes**](https://python.langchain.com/en/latest/modules/indexes.html)\n",
        "    - [Conceptual Guide](https://docs.langchain.com/docs/components/indexing)\n",
        "    - [Intro to indexes](https://python.langchain.com/en/latest/modules/indexes/getting_started.html)\n",
        "- [**Memory**](https://python.langchain.com/en/latest/modules/memory.html#)\n",
        "    - [Conceptual Guide](https://docs.langchain.com/docs/components/memory)\n",
        "    - [Intro into memory](https://python.langchain.com/en/latest/modules/memory/getting_started.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "wXqGhqZjxIUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prompts"
      ],
      "metadata": {
        "id": "QLBappahdcBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "go6dUlpMU32F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impersonation_template = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Rewrite the folowing text like Yoda: {text}\"\n",
        ")"
      ],
      "metadata": {
        "id": "fBx8KXZrVAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_template = PromptTemplate.from_template(\"Explain this objective like I am a child in 20 words. Start the sentence with the name of the objective: {objective}\")"
      ],
      "metadata": {
        "id": "yTq_Uz5HWDlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chains"
      ],
      "metadata": {
        "id": "M5XQ413hdpcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "Jv4w-sA-dr4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_chain = LLMChain(llm=llm, prompt=explain_template)"
      ],
      "metadata": {
        "id": "nFe8kIRweJds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explain_chain.run({\"objective\": \"Sky diving\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KE5CoX4recHL",
        "outputId": "58ca0e64-ace0-4fc3-eda4-f7358d192354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nSky diving is jumping out of a plane and using a parachute to float down to the ground.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "impersonation_chain = LLMChain(llm=llm, prompt=impersonation_template)"
      ],
      "metadata": {
        "id": "F92aeY8ngDSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "impersonation_chain.run(text=\"Life is like a bar of chocolate.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XgH8bPNBgVG-",
        "outputId": "d3fb9e9b-e466-4fe9-ad6f-ddc95f46bfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nA bar of chocolate, life is like.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "sequential_chain = SimpleSequentialChain(chains=[explain_chain, impersonation_chain], verbose=True)"
      ],
      "metadata": {
        "id": "opMbAvcne-U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_chain.run(\"mathematics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "9fL0YbzDgm30",
        "outputId": "add0df4f-05cc-4a62-b01a-dbd2f4c3b0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Mathematics is learning how to solve problems using numbers and shapes.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "Learning how to solve problems using numbers and shapes, mathematics is.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nLearning how to solve problems using numbers and shapes, mathematics is.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 - Basic components\n",
        "\n",
        "Let's create chain that will retrieve current information from wikipedia and translate them to french.\n",
        "\n",
        "[Wikipedia retriever docs](https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/wikipedia.html)"
      ],
      "metadata": {
        "id": "xjcxQiDah7hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ...live coding 🤞"
      ],
      "metadata": {
        "id": "OYVEgLdf98Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1 - Solution"
      ],
      "metadata": {
        "id": "xTpZKOzC99C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import WikipediaRetriever\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA, SimpleSequentialChain, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "retriever = WikipediaRetriever()\n",
        "\n",
        "chain_with_retriever = RetrievalQA.from_chain_type(llm,retriever=retriever)\n",
        "\n",
        "translate_template = PromptTemplate.from_template(\"Your task is to translate the following text to french: {text}\")\n",
        "translation_chain = LLMChain(llm=llm, prompt=translate_template)\n",
        "\n",
        "sequential_chain = SimpleSequentialChain(chains=[chain_with_retriever, translation_chain], verbose=True)\n",
        "\n",
        "sequential_chain.run(\"What is life?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ATk1rXWV55uN",
        "outputId": "a80b31f0-0cbe-4dd7-a733-fd6368543f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m \"What Is Life\" is a song by English rock musician George Harrison from his 1970 triple album All Things Must Pass. In many countries, it was issued as the second single from the album, in February 1971, becoming a top-ten hit in the United States, Canada and elsewhere, and topping singles charts in Australia and Switzerland. In the United Kingdom, \"What Is Life\" appeared as the B-side to \"My Sweet Lord\", which was the best-selling single there of 1971. Harrison's backing musicians on the song include Eric Clapton and the entire Delaney & Bonnie and Friends band, with whom he had toured during the final months of the Beatles. Harrison co-produced the recording with Phil Spector, whose Wall of Sound production also employed a prominent string arrangement by John Barham and multiple acoustic rhythm guitars, played by Harrison's fellow Apple Records signings Badfinger. An uptempo soul tune, \"What Is Life\" is one of several Harrison love songs that appear to be directed at both a woman and a deity.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "\"Qu'est-ce que la vie\" est une chanson du musicien de rock anglais George Harrison de son triple album de 1970 All Things Must Pass. Dans de nombreux pays, elle a été émise comme le deuxième single de l'album, en février 1971, devenant un succès dans le top-ten aux États-Unis, au Canada et ailleurs, et atteignant les charts de singles en Australie et en Suisse. Au Royaume-Uni, \"What Is Life\" est apparue comme le B-side de \"My Sweet Lord\", qui était le single le plus vendu là-bas en 1971. Les musiciens de soutien de Harrison sur la chanson comprennent Eric Clapton et toute la bande Delaney & Bonnie and Friends, avec qui il avait fait une tournée pendant les derniers mois des Beatles. Harrison a co-produit l'enregistrement avec Phil Spector, dont la production Wall of Sound comprenait également un arrangement de\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\"Qu\\'est-ce que la vie\" est une chanson du musicien de rock anglais George Harrison de son triple album de 1970 All Things Must Pass. Dans de nombreux pays, elle a été émise comme le deuxième single de l\\'album, en février 1971, devenant un succès dans le top-ten aux États-Unis, au Canada et ailleurs, et atteignant les charts de singles en Australie et en Suisse. Au Royaume-Uni, \"What Is Life\" est apparue comme le B-side de \"My Sweet Lord\", qui était le single le plus vendu là-bas en 1971. Les musiciens de soutien de Harrison sur la chanson comprennent Eric Clapton et toute la bande Delaney & Bonnie and Friends, avec qui il avait fait une tournée pendant les derniers mois des Beatles. Harrison a co-produit l\\'enregistrement avec Phil Spector, dont la production Wall of Sound comprenait également un arrangement de'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main exercise (15 min)"
      ],
      "metadata": {
        "id": "kBXr0thBAuoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom tools ReAct agents"
      ],
      "metadata": {
        "id": "HrUN8HYctyp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "import random"
      ],
      "metadata": {
        "id": "ZQ5sC1a7t1aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Home:\n",
        "  rooms = [\"living room\", \"bathroom\", \"bedroom\", \"kitchen\", \"library\"]\n",
        "  people = [\"wolfgang\", \"antonius\", \"augustus\", \"peter\"]\n",
        "\n",
        "  room_people_mapping = {}\n",
        "  people_room_mapping = {}\n",
        "  room_temperature = {}\n",
        "  room_windows = {}\n",
        "  room_lights = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    for person in self.people:\n",
        "      random_room = random.choice(self.rooms)\n",
        "      self.people_room_mapping[person] = random_room\n",
        "\n",
        "    for room in self.rooms:\n",
        "      self.room_people_mapping[room] = []\n",
        "      self.room_temperature[room] = random.randint(20, 26)\n",
        "      self.room_windows[room] = random.choice(['open', 'closed'])\n",
        "      self.room_lights[room] = random.choice(['on', 'off'])\n",
        "\n",
        "    for person, room in self.people_room_mapping.items():\n",
        "      self.room_people_mapping[room].append(person)\n",
        "\n",
        "  def _lowercase_input(func):\n",
        "    def wrapper(self, room_name):\n",
        "        if room_name is not None:\n",
        "          room_name = room_name.lower()\n",
        "          return func(self, room_name)\n",
        "        else:\n",
        "          func(self, \"\")\n",
        "    return wrapper\n",
        "\n",
        "  def _check_room_name(func):\n",
        "    def wrapper(self, room_name):\n",
        "        if room_name in self.rooms:\n",
        "          return func(self, room_name)\n",
        "        else:\n",
        "          return f\"There is no room named {room_name}\"\n",
        "    return wrapper\n",
        "\n",
        "  def _describe_home(self):\n",
        "    for room in self.rooms:\n",
        "      print(f\"\"\"Room: {room}\n",
        "            Lights: {self.room_lights[room]}\n",
        "            Temperature: {self.room_temperature[room]}\n",
        "            Windows: {self.room_windows[room]}\n",
        "            People present: {self.room_people_mapping[room]}\\n\"\"\")\n",
        "\n",
        "  def get_all_rooms(self, dummy_placeholder=\"\"):\n",
        "    return f\"Following rooms are present int the house: {self.rooms}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_all_people_in_room(self, room_name):\n",
        "    room_status = self.room_people_mapping.get(room_name)\n",
        "    return f\"In the {room_name} are following people: [{' ,'.join(room_status)}]\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  def find_person(self, person):\n",
        "    person_location = self.people_room_mapping.get(person)\n",
        "    if person_location == None:\n",
        "      return f\"There is no person named {person} in the house\"\n",
        "    return f\"{person} is in the {person_location}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_lights_status_in(self, room_name: str)-> str:\n",
        "    light_status = self.room_lights.get(room_name)\n",
        "    return f\"The lights in the {room_name} are {light_status}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_windows_status_in(self, room_name: str)-> str:\n",
        "    windows_status = self.room_windows.get(room_name)\n",
        "    return f\"The windows in the {room_name} are {windows_status}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_temperature_in(self, room_name: str)-> str:\n",
        "    temperature = self.room_temperature.get(room_name)\n",
        "    return f\"The tempereature in the {room_name} {temperature} °C\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def turn_on_lights_in(self, room_name):\n",
        "    self.room_lights[room_name] = 'on'\n",
        "    return f\"Turning on lights in {room_name}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def turn_off_lights_in(self, room_name):\n",
        "    self.room_lights[room_name] = 'off'\n",
        "    return f\"Turning off lights in {room_name}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def open_windows_in(self, room_name):\n",
        "    self.room_windows[room_name] = 'open'\n",
        "    return f\"Windows in the {room_name} are now opened\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def close_windows_in(self, room_name):\n",
        "    self.room_windows[room_name] = 'closed'\n",
        "    return f\"Windows in the {room_name} are now closed\""
      ],
      "metadata": {
        "id": "dniC3eIvt5zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_home = Home()"
      ],
      "metadata": {
        "id": "g-CGPX00t7O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_home._describe_home()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upg6tTKv5QgQ",
        "outputId": "029b6360-b3fb-4ad7-d505-504fa96219a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Room: living room\n",
            "            Lights: on\n",
            "            Temperature: 20\n",
            "            Windows: closed\n",
            "            People present: ['antonius', 'augustus']\n",
            "\n",
            "Room: bathroom\n",
            "            Lights: off\n",
            "            Temperature: 20\n",
            "            Windows: closed\n",
            "            People present: ['wolfgang']\n",
            "\n",
            "Room: bedroom\n",
            "            Lights: off\n",
            "            Temperature: 24\n",
            "            Windows: closed\n",
            "            People present: ['peter']\n",
            "\n",
            "Room: kitchen\n",
            "            Lights: off\n",
            "            Temperature: 20\n",
            "            Windows: closed\n",
            "            People present: []\n",
            "\n",
            "Room: library\n",
            "            Lights: on\n",
            "            Temperature: 25\n",
            "            Windows: closed\n",
            "            People present: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_home.get_all_rooms())\n",
        "print(my_home.get_all_people_in_room(\"living room\"))\n",
        "print(my_home.find_person(\"wolfgang\"))\n",
        "print(my_home.get_lights_status_in(\"living room\"))\n",
        "print(my_home.get_temperature_in(\"living room\"))\n",
        "print(my_home.get_lights_status_in(\"living room\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1b1zKc62_tJ",
        "outputId": "040c82ac-3601-4645-8242-42c3a4c98b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following rooms are present int the house: ['living room', 'bathroom', 'bedroom', 'kitchen', 'library']\n",
            "In the living room are following people: [antonius ,augustus]\n",
            "wolfgang is in the bathroom\n",
            "The lights in the living room are on\n",
            "The tempereature in the living room 20 °C\n",
            "The lights in the living room are on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=my_home.get_all_people_in_room,\n",
        "        name = \"Get people in room\",\n",
        "        description=\"useful for when you need to find out what people are located in specific room as well as finding out whether specific room is empty or not. takes name of the room as parameter\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.get_all_rooms,\n",
        "        name = \"List all rooms\",\n",
        "        description=\"useful for when you need to find out what rooms are available in home. The function returns a list of available rooms\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.find_person,\n",
        "        name = \"Find person\",\n",
        "        description=\"useful for when you need to find where is certain person in the home. The input for this function is the individual room. (ex: living room, bathroom)\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.turn_on_lights_in,\n",
        "        name=\"Turn on lights in specific room\",\n",
        "        description=\"useful for turning on the light in specific room. The input for this function is the individual room. (ex: living room, bathroom)\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.turn_off_lights_in,\n",
        "        name=\"Turn off lights in specific room\",\n",
        "        description=\"useful for turning off the light in specific room. The input for this function is the individual room. (ex: living room, bathroom)\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.get_lights_status_in,\n",
        "        name=\"Get status[on/off] lights in specific room\",\n",
        "        description=\"useful for finding out if the lights are on / off in specific room. The input for this function is the individual room. (ex: living room, bathroom)\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "nCmNi4dwwu3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.25)\n",
        "\n",
        "home_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ],
      "metadata": {
        "id": "fIsrLMtTyJhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "home_agent.run(\"Where is Wolfgang?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "HKk6Ho5ay7UY",
        "outputId": "0aaba9c8-62bb-4847-dd29-ea36dd0fcd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out where Wolfgang is located.\n",
            "Action: Find person\n",
            "Action Input: Wolfgang\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mwolfgang is in the kitchen\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Wolfgang is in the kitchen.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wolfgang is in the kitchen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "home_agent.run(\"trn of light in he kitchn\") # intentional typos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "A4HP9pMu1bVo",
        "outputId": "213958fe-b5b7-421c-8b7a-9554dd77d96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to turn off the lights in the kitchen\n",
            "Action: Turn off lights in specific room\n",
            "Action Input: kitchen\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mTurning off lights in kitchen\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Lights in the kitchen have been turned off.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lights in the kitchen have been turned off.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "home_agent.run(\"If there is someone in the living room, you should turn on the lights, if not, do not turn on the lights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "01qmeUgez-CA",
        "outputId": "ef07f2e2-5f5f-4679-dada-9c7b611ee6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out if someone is in the living room.\n",
            "Action: Get people in room\n",
            "Action Input: Living room\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the living room are following people: []\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m There is no one in the living room.\n",
            "Action: Turn off lights in specific room\n",
            "Action Input: Living room\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mTurning off lights in living room\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: Do not turn on the lights in the living room.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do not turn on the lights in the living room.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "home_agent.run(\"Is there somone in the house?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "_sWx8XZ7np3C",
        "outputId": "e7ac69b9-860c-4833-a724-acb6a7fc8720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who is in the house.\n",
            "Action: Get people in room\n",
            "Action Input: All rooms\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThere is no room named all rooms\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out which rooms are available\n",
            "Action: List all rooms\n",
            "Action Input: N/A\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mFollowing rooms are present int the house: ['living room', 'bathroom', 'bedroom', 'kitchen', 'library']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to check if anyone is in any of these rooms\n",
            "Action: Find person\n",
            "Action Input: living room, bathroom, bedroom, kitchen, library\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mThere is no person named living room, bathroom, bedroom, kitchen, library in the house\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to check if anyone is in any of these rooms\n",
            "Action: Get people in room\n",
            "Action Input: living room, bathroom, bedroom, kitchen, library\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThere is no room named living room, bathroom, bedroom, kitchen, library\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to check each room individually\n",
            "Action: Get people in room\n",
            "Action Input: living room\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the living room are following people: [antonius ,augustus]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Yes, there are people in the house.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, there are people in the house.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "home_agent.run(\"Find all people in the house\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "eAIX7qRlHpeG",
        "outputId": "27e22322-e314-4b0e-be9a-e5192686c8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out which rooms are available and then find out who is in each room\n",
            "Action: List all rooms\n",
            "Action Input: N/A\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mFollowing rooms are present int the house: ['living room', 'bathroom', 'bedroom', 'kitchen', 'library']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out who is in each room\n",
            "Action: Get people in room\n",
            "Action Input: living room\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the living room are following people: []\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to repeat this for all the rooms\n",
            "Action: Get people in room\n",
            "Action Input: bathroom\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the bathroom are following people: []\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to repeat this for all the rooms\n",
            "Action: Get people in room\n",
            "Action Input: bedroom\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the bedroom are following people: [antonius ,peter]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to repeat this for all the rooms\n",
            "Action: Get people in room\n",
            "Action Input: kitchen\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the kitchen are following people: [wolfgang ,augustus]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to repeat this for all the rooms\n",
            "Action: Get people in room\n",
            "Action Input: library\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mIn the library are following people: []\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The people in the house are antonius, peter, wolfgang, and augustus.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The people in the house are antonius, peter, wolfgang, and augustus.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 - Agents and their Tools\n",
        "\n",
        "Add support for window and temperature management for queries like:\n",
        "- \"If the temperature in the living room is less than 21, close the windows\""
      ],
      "metadata": {
        "id": "amauMHF9DPvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2 - Solution"
      ],
      "metadata": {
        "id": "W6p6Q2ZwJxxD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FXizEXiHL3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: Custom agent with custom tools"
      ],
      "metadata": {
        "id": "WREnSfb78PQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
        "from typing import List, Union\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
        "import re"
      ],
      "metadata": {
        "id": "w82u3-eG8TJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Home:\n",
        "  rooms = [\"living room\", \"bathroom\", \"bedroom\", \"kitchen\", \"library\"]\n",
        "  people = [\"wolfgang\", \"antonius\", \"augustus\", \"peter\"]\n",
        "\n",
        "  room_people_mapping = {}\n",
        "  people_room_mapping = {}\n",
        "  room_temperature = {}\n",
        "  room_windows = {}\n",
        "  room_lights = {}\n",
        "\n",
        "  def __init__(self):\n",
        "    for person in self.people:\n",
        "      random_room = random.choice(self.rooms)\n",
        "      self.people_room_mapping[person] = random_room\n",
        "\n",
        "    for room in self.rooms:\n",
        "      self.room_people_mapping[room] = []\n",
        "      self.room_temperature[room] = random.randint(20, 26)\n",
        "      self.room_windows[room] = random.choice(['open', 'closed'])\n",
        "      self.room_lights[room] = random.choice(['on', 'off'])\n",
        "\n",
        "    for person, room in self.people_room_mapping.items():\n",
        "      self.room_people_mapping[room].append(person)\n",
        "\n",
        "  def _lowercase_input(func):\n",
        "    def wrapper(self, room_name):\n",
        "        if room_name is not None:\n",
        "          room_name = room_name.lower()\n",
        "          return func(self, room_name)\n",
        "        else:\n",
        "          func(self, \"\")\n",
        "    return wrapper\n",
        "\n",
        "  def _check_room_name(func):\n",
        "    def wrapper(self, room_name):\n",
        "        if room_name in self.rooms:\n",
        "          return func(self, room_name)\n",
        "        else:\n",
        "          return f\"There is no room named {room_name}\"\n",
        "    return wrapper\n",
        "\n",
        "  def _describe_home(self):\n",
        "    for room in self.rooms:\n",
        "      print(f\"\"\"Room: {room}\n",
        "            Lights: {self.room_lights[room]}\n",
        "            Temperature: {self.room_temperature[room]}\n",
        "            Windows: {self.room_windows[room]}\n",
        "            People present: {self.room_people_mapping[room]}\\n\"\"\")\n",
        "\n",
        "  def get_all_rooms(self, dummy_placeholder=\"\"):\n",
        "    return f\"Following rooms are present int the house: {self.rooms}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_all_people_in_room(self, room_name):\n",
        "    room_status = self.room_people_mapping.get(room_name)\n",
        "    return f\"In the {room_name} are following people: [{' ,'.join(room_status)}]\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  def find_person(self, person):\n",
        "    person_location = self.people_room_mapping.get(person)\n",
        "    if person_location == None:\n",
        "      return f\"There is no person named {person} in the house\"\n",
        "    return f\"{person} is in the {person_location}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_lights_status_in(self, room_name: str)-> str:\n",
        "    light_status = self.room_lights.get(room_name)\n",
        "    return f\"The lights in the {room_name} are {light_status}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_windows_status_in(self, room_name: str)-> str:\n",
        "    windows_status = self.room_windows.get(room_name)\n",
        "    return f\"The windows in the {room_name} are {windows_status}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def get_temperature_in(self, room_name: str)-> str:\n",
        "    temperature = self.room_temperature.get(room_name)\n",
        "    return f\"The tempereature in the {room_name} {temperature} °C\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def turn_on_lights_in(self, room_name):\n",
        "    self.room_lights[room_name] = 'on'\n",
        "    return f\"Turning on lights in {room_name}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def turn_off_lights_in(self, room_name):\n",
        "    self.room_lights[room_name] = 'off'\n",
        "    return f\"Turning off lights in {room_name}\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def open_windows_in(self, room_name):\n",
        "    self.room_windows[room_name] = 'open'\n",
        "    return f\"Windows in the {room_name} are now opened\"\n",
        "\n",
        "  @_lowercase_input\n",
        "  @_check_room_name\n",
        "  def close_windows_in(self, room_name):\n",
        "    self.room_windows[room_name] = 'closed'\n",
        "    return f\"Windows in the {room_name} are now closed\""
      ],
      "metadata": {
        "id": "StdGpyUU8ZLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=my_home.get_all_people_in_room,\n",
        "        name = \"Search people\",\n",
        "        description=\"useful for when you need to find out what people are located in specific romm as well as finding out wheter specific room is empty or not. takes name of the room as a parameter\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.get_all_rooms,\n",
        "        name = \"List rooms\",\n",
        "        description=\"useful for when you need to find out what rooms are available in home. The input for this function is the individual room\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.find_person,\n",
        "        name = \"Find occupant\",\n",
        "        description=\"useful for when you need to find in which room a person is\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.turn_on_lights_in,\n",
        "        name=\"Turn on lights in specific room\",\n",
        "        description=\"useful for turning on the light in specific room\"\n",
        "    ),\n",
        "    Tool.from_function(\n",
        "        func=my_home.turn_off_lights_in,\n",
        "        name=\"Turn off lights in specific room\",\n",
        "        description=\"useful for turning off the light in specific room\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "HOiDpDpM8ZYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)"
      ],
      "metadata": {
        "id": "V15g6Z0r8d2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the base template\n",
        "template = \"\"\"You are the home automaition agent that helps users with gathering information about\n",
        "their homes as wella s heling them with automation. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "This is the history of your conversation with the human:\n",
        "\n",
        "{chat_history}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Comand: the input command you must answer / make action\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final: the final answer / action to the original input command\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\""
      ],
      "metadata": {
        "id": "d6uYZfDK8uUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = CustomPromptTemplate(\n",
        "    template=template,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\", \"chat_history\"]\n",
        ")"
      ],
      "metadata": {
        "id": "S4OzT2RX_NJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOutputParser(AgentOutputParser):\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
      ],
      "metadata": {
        "id": "CaVsVpgc_PZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = CustomOutputParser()"
      ],
      "metadata": {
        "id": "_JL9hcOk_Tby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "readonlymemory = ReadOnlySharedMemory(memory=memory)"
      ],
      "metadata": {
        "id": "-jCPpq_rX2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.25)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=readonlymemory\n",
        ")"
      ],
      "metadata": {
        "id": "d1dbECx4_Wk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_names = [tool.name for tool in tools]\n",
        "\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "i1ciJAOa_ZPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    max_iterations=5\n",
        ")"
      ],
      "metadata": {
        "id": "IA4TISEG_b7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"Where is Augustus?\")"
      ],
      "metadata": {
        "id": "djwFAHH0_iZh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "6ed999d5-1404-4b9e-8a7f-31ce1ce1f6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: I need to find out where Augustus is located.\n",
            "Action: Find occupant\n",
            "Action Input: Augustus\u001b[0m\n",
            "\n",
            "Observation:\u001b[38;5;200m\u001b[1;3maugustus is in the bathroom\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final: Augustus is in the bathroom\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I now know the final answer\\nFinal: Augustus is in the bathroom'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent_executor.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNZ2giG0elvi",
        "outputId": "302d2230-7f7d-4378-a55a-9a22aaea0f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Where is Augustus?\n",
            "AI: I now know the final answer\n",
            "Final: Augustus is in the bathroom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.run(\"Turn off the lights in that room where is augustus?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "rqbTDyLBY_CF",
        "outputId": "19bbca28-7ad8-4620-8e53-c61303b45702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to find out which room Augustus is in and then turn off the lights in that room\n",
            "Action: Find occupant\n",
            "Action Input: Augustus\u001b[0m\n",
            "\n",
            "Observation:\u001b[38;5;200m\u001b[1;3mkitchen\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I now need to turn off the light in the kitchen \n",
            "Action: Turn off lights in specific room\n",
            "Action Input: kitchen\u001b[0m\n",
            "\n",
            "Observation:\u001b[33;1m\u001b[1;3mTurning off lights in kitchen\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I now know the lights are off in the kitchen\n",
            "Final: Lights are off in the kitchen.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I now know the lights are off in the kitchen\\nFinal: Lights are off in the kitchen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional exercise (10 min)"
      ],
      "metadata": {
        "id": "nqKx26iSA71B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 - Document Q&A\n",
        "\n",
        "Let's create a system that that answers questions from some very long texts(books, movie scripts, software documentations, meeting transcriptions, etc.)\n",
        "\n",
        "Hint - the solution should contain these parts:\n",
        "- [Loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) - loads your files into Documents\n",
        "- [Splitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) - splits your long text into chunks\n",
        "- [Embedding model](https://python.langchain.com/en/latest/modules/models/text_embedding.html) - converts your short texts into embeddings\n",
        "- [Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html) - database for your embeddings\n",
        "- [RetrievalQA chain](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html) - the main 🦜⛓️ part"
      ],
      "metadata": {
        "id": "CNn5_2UYJ88f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3 - Solution\n",
        "\n",
        "Sources:\n",
        "- [IMSdb loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/imsdb.html)\n",
        "- [Recursive Text Splitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)\n",
        "- [FAISS Vectorstore](https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/faiss.html)\n",
        "- [Question asnwering chain](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html)\n",
        "- [Retrieval and QA chain](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html)\n"
      ],
      "metadata": {
        "id": "GlgVZxveJ6lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loader\n",
        "\n",
        "Fetch the PulpFiction movie script from [IMSdb](https://imsdb.com)"
      ],
      "metadata": {
        "id": "yCPgOASgAMRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import IMSDbLoader\n",
        "\n",
        "loader = IMSDbLoader(\"https://imsdb.com/scripts/Pulp-Fiction.html\")\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "print(len(documents))"
      ],
      "metadata": {
        "id": "73D4onqVzjKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dae0bb-282d-4f77-ef33-59d200ad673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see here that the whole script contains 304 709 characters."
      ],
      "metadata": {
        "id": "6v6YmMb0OQBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documents[0]))\n",
        "print(len(documents[0].page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1V6Pt7p26cG",
        "outputId": "e99cac42-a335-492d-df78-d3da512321f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain.schema.Document'>\n",
            "304709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script contains a lot of empty spaces which we will remove here.\n",
        "\n",
        "However the remaining lenght of the script is still very long, 169 541 characters long."
      ],
      "metadata": {
        "id": "o8SN_m9ROVyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "\n",
        "cleanText = documents[0].page_content.replace(\"  \", \"\")\n",
        "document = Document(page_content=cleanText, metadata=documents[0].metadata)\n",
        "\n",
        "print(len(document.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JxpPC8z7XmV",
        "outputId": "9b5bf911-f326-47bd-95b9-0dc146185788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitter\n",
        "\n",
        "We simply split the whole script into smaller chunks of 1000 characters that overlap by 200 characters.\n",
        "\n",
        "This is a very lazy and naive way of splitting long documents. We might accidentaly lose important information that spans across 1000 characters which will make the question asnwering more difficult.\n",
        "\n",
        "More sophisticated approach would be to split the movie script into some logical sequences like scenes, shots, dialogues."
      ],
      "metadata": {
        "id": "oFR8n-czAQsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
        "\n",
        "docs = text_splitter.split_documents([document])\n",
        "\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74Ljx2gumiy",
        "outputId": "8296e0c1-0d94-4cb8-ab47-408d9fa8d85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "213"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding model & Vectorstore\n",
        "\n",
        "This sections embeds the documents with the default OpenAI encoder model and saves their embeddings into a [FAISS](https://github.com/facebookresearch/faiss) db (langchain calls it a vectorstore)."
      ],
      "metadata": {
        "id": "PjanYi4e19MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "mTrZTaBxzdkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is just a double check that the embeddings are correct and the database works.\n",
        "\n",
        "If we take any line from the script, this section should find the document that contains the line."
      ],
      "metadata": {
        "id": "jqg2vd7y2XzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Uncomfortable silences. Why do we feel it's necessary to yak about bullshit in order to be comfortable?\"\n",
        "results = db.similarity_search_with_score(query, k=1)\n",
        "\n",
        "for doc, score in results:\n",
        "  print(score)\n",
        "  print(doc.page_content)"
      ],
      "metadata": {
        "id": "kScgVu2K2QrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839de2ea-a68d-4bf5-de20-60ec1863a275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.34167325\n",
            "kooties.\r\n",
            "\r\n",
            " Vincent smiles.\r\n",
            "\r\n",
            " VINCENT\r\n",
            " Yeah, but maybe I do.\r\n",
            "\r\n",
            " MIA\r\n",
            " Kooties I can handle.\r\n",
            "\r\n",
            " He takes a sip.\r\n",
            "\r\n",
            " VINCENT\r\n",
            " Goddamn! That's a pretty fuckin' \r\n",
            " good milk shake.\r\n",
            "\r\n",
            " MIA\r\n",
            " Told ya.\r\n",
            "\r\n",
            " VINCENT\r\n",
            " I don't know if it's worth five \r\n",
            " dollars, but it's pretty fuckin' \r\n",
            " good.\r\n",
            "\r\n",
            " He slides the shake back.\r\n",
            "\r\n",
            " Then the first of an uncomfortable silence happens.\r\n",
            "\r\n",
            " MIA\r\n",
            " Don't you hate that?\r\n",
            "\r\n",
            " VINCENT\r\n",
            " What?\r\n",
            "\r\n",
            " MIA\r\n",
            " Uncomfortable silences. Why do we \r\n",
            " feel it's necessary to yak about \r\n",
            " bullshit in order to be comfortable?\r\n",
            "\r\n",
            " VINCENT\r\n",
            " I don't know.\r\n",
            "\r\n",
            " MIA\r\n",
            " That's when you know you found \r\n",
            " somebody special. When you can just \r\n",
            " shit the fuck up for a minute, and \r\n",
            " comfortably share silence.\r\n",
            "\r\n",
            " VINCENT\r\n",
            " I don't think we're there yet. But \r\n",
            " don't feel bad, we just met each \r\n",
            " other.\r\n",
            "\r\n",
            " MIA\r\n",
            " Well I'll tell you what, I'll go to \r\n",
            " the bathroom and powder my nose, \r\n",
            " while you sit here and think of \r\n",
            " something to say.\r\n",
            "\r\n",
            " VINCENT\r\n",
            " I'll do that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RetrievalQA chain\n",
        "\n",
        "This is the part where the answering happens.\n",
        "\n",
        "The RetrievalQA chain firstly finds the 4 most relevant documents to our query and sends it to the LLM as a context. The LLM is then asked to find an answer in this context."
      ],
      "metadata": {
        "id": "UwNuzMLt2czm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)"
      ],
      "metadata": {
        "id": "bOIIn_lpWs-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example questions"
      ],
      "metadata": {
        "id": "NQsBMGA-Q7qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is a quarter pounder with cheese called in france?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4QfInZo5WlCu",
        "outputId": "67b44970-92c9-42a1-9916-462ad0af958b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A quarter pounder with cheese is called a Royale with Cheese in France.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What do they put on fries in holland?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hj6mkEN7Wk_a",
        "outputId": "1070f6ed-62cb-4940-fe84-6d9490407128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' They put mayonnaise on fries in Holland.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the passage from bible that Jules has memorized?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "6ziF_k4IzdiL",
        "outputId": "e869b871-51c5-4da6-c3ec-595ac57c6185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Ezekiel 25:17. \"The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness, for he is truly his brother\\'s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Where do Vincent and Mia have dinner?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oXBM3RYELCff",
        "outputId": "6388872c-96f0-483b-da6b-e75744bf0429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Vincent and Mia have dinner at Jackrabbit Slim's.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the name of the goon that Vincent accidentally shoots in the face?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N7S8nj_4L3Px",
        "outputId": "0d409126-ee7d-47ae-8b6d-be521b0ed9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Marvin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}